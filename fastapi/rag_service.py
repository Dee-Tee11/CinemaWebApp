import os
import json
import requests
from typing import List, Dict, Optional
import time
import random

class GroqClient:
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.url = "https://api.groq.com/openai/v1/chat/completions"
        self.model = "llama-3.1-8b-instant"  # Updated to faster/higher limit model
        
    def generate(self, prompt: str, max_tokens: int = 2048) -> str:
        # User requested delay to avoid rate limits
        delay = random.randint(3, 8)
        print(f"   ⏳ Pausa técnica de {delay}s antes de chamar o LLM...")
        time.sleep(delay)

        payload = {
            "model": self.model,
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.5,
            "max_tokens": max_tokens
        }
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        max_retries = 3
        
        for attempt in range(max_retries):
            try:
                response = requests.post(self.url, json=payload, headers=headers, timeout=30)
                
                if response.status_code == 200:
                    data = response.json()
                    return data['choices'][0]['message']['content']
                
                elif response.status_code == 429:
                    wait_time = 10 * (attempt + 1)
                    print(f"Update: ⚠️ Rate limit atingido. Pausa de {wait_time}s... (Tentativa {attempt+1}/{max_retries})")
                    time.sleep(wait_time)
                    continue
                    
                else:
                    print(f"❌ Groq API Error: {response.text}")
                    return ""
                    
            except Exception as e:
                print(f"❌ LLM Call Failed: {e}")
                return ""
        
        return ""

class RagService:
    def __init__(self):
        self.api_key = os.getenv("GROQ_API_KEY")
        if not self.api_key:
            print("⚠️ GROQ_API_KEY missing! RAG features disabled.")
            self.llm = None
        else:
            self.llm = GroqClient(self.api_key)
            
    def rerank(self, ratings: List[Dict], candidates: List[Dict], user_query: str = "") -> List[Dict]:
        """
        Primary RAG Strategy (Direct): Skip Persona, provide Raw History.
        """
        # Format History
        history_text = "\n".join([f"- {r['title']} ({r['rating']}⭐)" for r in ratings if r['rating'] >= 15])
        
        # Format Candidates
        candidates_text = ""
        for i, c in enumerate(candidates):
            candidates_text += f"ID {i}: {c['title']} ({c.get('year', 'N/A')}) - {c.get('genre', 'N/A')}\n"
            candidates_text += f"   Overview: {c.get('overview', 'N/A')[:150]}...\n"
        
        prompt = f"""You are a movie recommendation engine.
        
USER LIKES THESE MOVIES:
{history_text}

CANDIDATES (generated by algorithm):
{candidates_text}

TASK:
Re-rank the CANDIDATES list to find the best 5-8 matches for the user based *exclusively* on their movie history.
The user did NOT search for anything specific, so your goal is to find "more like what they already like".

CRITICAL RULES:
1. ONLY SELECT FROM THE CANDIDATES LIST. DO NOT INVENT MOVIES.
2. If a candidate has a low rank (high ID) but matches the user history perfectly, boost it!
3. If a candidate matches the genre/tone of 'USER LIKES', it is a good match.
4. If a candidate is completely unrelated, ignore it (do not return it).

OUTPUT FORMAT:
Provide a valid JSON list. NO COMMENTS. NO MATH.
[
  {{
    "index": 0,
    "adjusted_score": 0.95,
    "reason": "Perfect match because..."
  }}
]
"""
        response = self.llm.generate(prompt)
        
        # Parse Response
        try:
            if '[' in response and ']' in response:
                start = response.find('[')
                end = response.rfind(']') + 1
                json_str = response[start:end]
                lines = [l.split('//')[0] for l in json_str.split('\n')]
                json_clean = '\n'.join(lines)
                
                decisions = json.loads(json_clean)
                
                reranked = []
                for d in decisions:
                    idx = d.get('index')
                    if idx is not None and idx < len(candidates):
                        cand = candidates[idx]
                        cand['rag_explanation'] = d.get('reason', 'N/A')
                        cand['score'] = float(d.get('adjusted_score', cand['score']))
                        reranked.append(cand)
                
                reranked.sort(key=lambda x: x['score'], reverse=True)
                return reranked if reranked else candidates[:10]
        except Exception as e:
            print(f"❌ JSON Parsing failed: {e}")
            return candidates[:10]
        return candidates[:10]

    def chat_with_history(self, ratings: List[Dict], user_message: str) -> str:
        """
        Chatbot mode: LLM has access to full user history + user message.
        Returns a conversational text response.
        """
        # Format history very compactly to save tokens
        history_len = len(ratings)
        # Prioritize top rated movies
        top_rated = sorted(ratings, key=lambda x: x['rating'], reverse=True)[:50]
        
        history_text = "\n".join([f"- {r['title']} ({r['rating']}⭐)" for r in top_rated])
        
        prompt = f"""You are a personalized movie expert assistant.
        
USER PROFILE ({history_len} movies total, top 50 shown):
{history_text}

USER MESSAGE: "{user_message}"

TASK:
Answer the user's message based on their movie taste.
- Be helpful, conversational, and concise.
- Use their history to justify your answers (e.g., "Since you liked X...").
- If they ask for recommendations, suggest 3-5 titles that aren't in their history.
- Do NOT output JSON. Output normal text.
"""
        try:
             # Use a slightly higher temperature for chat to be more creative? Default is fine.
             response = self.llm.generate(prompt, max_tokens=1024)
             return response.strip()
        except Exception as e:
             return f"⚠️ Error generating response: {e}"
